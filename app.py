import gradio as gr
import cv2
from PIL import Image
import numpy as np
import torch
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler
from controlnet_aux import OpenposeDetector
import os

# å…¨åŸŸè®Šæ•¸å„²å­˜æ¨¡å‹ï¼Œé¿å…é‡è¤‡è¼‰å…¥
openpose = None
pipe = None

def load_models():
    global openpose, pipe
    if openpose is None:
        print("æ­£åœ¨è¼‰å…¥ OpenPose åµæ¸¬å™¨...")
        openpose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
    
    if pipe is None:
        print("æ­£åœ¨è¼‰å…¥ ControlNet å’Œ Stable Diffusion Inpainting æ¨¡å‹...")
        controlnet = ControlNetModel.from_pretrained(
            "fusing/stable-diffusion-v1-5-controlnet-openpose", 
            torch_dtype=torch.float16
        )

        # ä½¿ç”¨ Inpainting Pipeline ä¾†ä¿ç•™åŸåœ–çš„æœªé®ç½©å€åŸŸ (å¦‚è‡‰éƒ¨ã€èƒŒæ™¯)
        pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
            "runwayml/stable-diffusion-inpainting",
            controlnet=controlnet,
            torch_dtype=torch.float16,
        )

        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
        pipe.enable_model_cpu_offload()
        
        try:
            pipe.enable_xformers_memory_efficient_attention()
        except Exception as e:
            print("æœªå•Ÿç”¨ xformers (å¯é¸):", e)
    
    return "æ¨¡å‹è¼‰å…¥å®Œæˆï¼"

def generate_fashion(input_dict, prompt, negative_prompt, num_steps, seed):
    if input_dict is None or input_dict["background"] is None:
        return None, None
    
    image = input_dict["background"]
    
    # è™•ç†é®ç½©ï¼šå¾ ImageEditor çš„åœ–å±¤ä¸­æå–
    if not input_dict["layers"]:
        # å¦‚æœæ²’æœ‰åœ–å±¤ï¼Œå˜—è©¦çœ‹çœ‹æ˜¯å¦æœ‰ composite (æœ‰äº›ç‰ˆæœ¬è¡Œç‚ºä¸åŒ)
        # ä½†é€šå¸¸ layers æœƒæœ‰å¡—æŠ¹å…§å®¹
        return None, None
        
    # åˆä½µæ‰€æœ‰åœ–å±¤çš„ Alpha é€šé“ä½œç‚ºé®ç½©
    mask = Image.new("L", image.size, 0)
    for layer in input_dict["layers"]:
        # layer æ˜¯ RGBAï¼Œå–å‡º Alpha é€šé“
        layer_alpha = layer.split()[-1]
        mask = Image.fromarray(np.maximum(np.array(mask), np.array(layer_alpha)))

    if image is None:
        return None, None
    
    if pipe is None or openpose is None:
        load_models()

    # 1. æå–å§¿å‹¢
    print("æ­£åœ¨æå–å§¿å‹¢...")
    # ç¢ºä¿åœ–ç‰‡æ˜¯ PIL Image æ ¼å¼
    if isinstance(image, np.ndarray):
        image = Image.fromarray(image)
    
    # èª¿æ•´å¤§å°ä»¥ç¬¦åˆæ¨¡å‹éœ€æ±‚ (å»ºè­° 512x512 æˆ–å…¶å€æ•¸)
    w, h = image.size
    # ç°¡å–®ç¸®æ”¾è‡³ 512x512 é€²è¡Œè™•ç†
    process_image = image.resize((512, 512)).convert("RGB")
    process_mask = mask.resize((512, 512)).convert("RGB") # è½‰æ›ç‚º RGB ä»¥é¿å…æŸäº›ç‰ˆæœ¬çš„ç›¸å®¹æ€§å•é¡Œ
        
    pose_image = openpose(process_image)

    # 2. ç”Ÿæˆåœ–ç‰‡
    print(f"æ­£åœ¨ç”Ÿæˆåœ–ç‰‡: {prompt}")
    generator = torch.Generator(device="cpu").manual_seed(int(seed))
    
    output = pipe(
        prompt,
        image=process_image,
        mask_image=process_mask,
        control_image=pose_image,
        negative_prompt=negative_prompt,
        generator=generator,
        num_inference_steps=int(num_steps),
        height=512,
        width=512,
    )
    
    generated_image = output.images[0]
    
    # å°‡ç”Ÿæˆçš„åœ–ç‰‡ resize å›åŸå§‹å¤§å° (å¯é¸)
    generated_image = generated_image.resize((w, h))
    pose_image = pose_image.resize((w, h))
    
    return pose_image, generated_image

# å®šç¾© Gradio ä»‹é¢
with gr.Blocks(title="AI æ™‚å°šè¨­è¨ˆå¸«") as demo:
    gr.Markdown("# ğŸ‘— AI æ™‚å°šè¨­è¨ˆå¸« (è™›æ“¬è©¦ç©¿ç‰ˆ)")
    gr.Markdown("ä¸Šå‚³äººç‰©ç…§ç‰‡ï¼Œä¸¦**ä½¿ç”¨ç•«ç­†å¡—æŠ¹æƒ³è¦æ›´æ›çš„è¡£æœå€åŸŸ**ï¼ŒAI å°‡ç‚ºä½ ç”Ÿæˆå…¨æ–°çš„æ™‚å°šç©¿æ­ï¼ŒåŒæ™‚ä¿ç•™æ¨¡ç‰¹å…’çš„è‡‰éƒ¨å’Œèº«é«”ç‰¹å¾µï¼")
    
    with gr.Row():
        with gr.Column():
            # ä½¿ç”¨ ImageEditor è®“ä½¿ç”¨è€…å¯ä»¥å¡—æŠ¹é®ç½©
            input_img = gr.ImageEditor(label="ä¸Šå‚³ç…§ç‰‡ä¸¦å¡—æŠ¹è¡£æœå€åŸŸ", type="pil")
            prompt_text = gr.Textbox(
                label="æœè£æè¿° (Prompt)", 
                placeholder="ä¾‹å¦‚: a fashion model wearing a red silk evening gown, runway photography...",
                value="a fashion model wearing a red silk evening gown, runway photography, high fashion, 8k, highly detailed"
            )
            neg_prompt_text = gr.Textbox(
                label="è² é¢æè¿° (Negative Prompt)", 
                value="monochrome, lowres, bad anatomy, worst quality, low quality, missing limbs, extra limbs"
            )
            with gr.Accordion("é€²éšè¨­å®š", open=False):
                steps_slider = gr.Slider(minimum=10, maximum=50, value=20, step=1, label="æ¨è«–æ­¥æ•¸ (Steps)")
                seed_number = gr.Number(value=42, label="éš¨æ©Ÿç¨®å­ (Seed)")
            
            run_btn = gr.Button("é–‹å§‹ç”Ÿæˆ", variant="primary")
        
        with gr.Column():
            with gr.Row():
                pose_output = gr.Image(label="åµæ¸¬åˆ°çš„éª¨æ¶ (Pose)")
                final_output = gr.Image(label="ç”Ÿæˆçµæœ")

    # ç¶å®šäº‹ä»¶
    run_btn.click(
        fn=generate_fashion,
        inputs=[input_img, prompt_text, neg_prompt_text, steps_slider, seed_number],
        outputs=[pose_output, final_output]
    )

    # å•Ÿå‹•æ™‚é å…ˆè¼‰å…¥æ¨¡å‹ (å¯é¸ï¼Œè‹¥ä¸æƒ³å•Ÿå‹•æ™‚å¡ä½å¯è¨»è§£æ‰)
    # load_models()

if __name__ == "__main__":
    demo.launch()
